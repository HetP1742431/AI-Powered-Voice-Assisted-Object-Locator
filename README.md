# ğŸ¯ AI-powered Voice-Assisted Object Locator (AIVOL)

> ğŸ‘ï¸ Designed to assist visually impaired individuals in locating everyday objects using voice commands, real-time computer vision, and natural language processing â€” all running on a Raspberry Pi 5.

---

## ğŸ‘¥ Team Members

- Het Bharatkumar Patel (Maintainer of this fork)
- Parth Dadhania
- Chinmoy Sahoo
- Dang Nguyen

---

## ğŸ”‘ Core Features

âœ… Wake word functionality (â€œHey Assistantâ€) to trigger voice command flow  
âœ… End-to-end integration of speech recognition, object detection, text-to-speech and real-time feedback  
âœ… Real-time object locating and directional guidance  
âœ… NLP-powered query understanding (e.g., â€œWhere is my red cup on the table?â€)  
âœ… Handles missing objects with fallback suggestions  
âœ… Lightweight and plug-and-play setup  
âœ… Works seamlessly on both WSL and Raspberry Pi OS

---

## ğŸ§  How It Works

1. **Wake Word Detection** â€“ Listens for the keyword â€œHey AIVOLâ€ before capturing commands
2. **Voice Recognition** â€“ Captures userâ€™s voice query using Google Speech Recognition  
3. **NLP Parser** â€“ Processes the text to extract object descriptors (name, color, location) using spaCy/NLTK  
4. **Object Detection** â€“ Scans the environment using YOLOv5m with Objects365 dataset  
5. **Filtering & Reasoning** â€“ Filters objects based on query relevance and WordNet synonyms
6. **Text-to-Speech** â€“ Provides real-time guidance (e.g., "Move slightly left") using `gTTS`
7. **Continuous Guidance** â€“ Tracks movement and updates directions while user moves using Mediapipe

---

## ğŸ“½ï¸ Working Demo

ğŸ¥ **Watch the full working prototype demo here:**  
[ğŸ“º AIVOL Demo Video](https://drive.google.com/file/d/12i0sUYGtml0EsvSWSRfKyyfh2CM3sd4P/view?usp=sharing)

---

## ğŸ§° Tech Stack

| Layer | Technologies |
|------|--------------|
| ğŸ’» Hardware | Raspberry Pi 5 (16GB RAM, 128GB Storage), 120Â° FoV USB Camera, Logitech USB Headset |
| ğŸ§  AI Models | YOLOv5m (trained on Objects365), Mediapipe |
| ğŸ—£ï¸ Voice | Google Speech Recognition, PyAudio, gTTS |
| ğŸ§¾ NLP | spaCy, NLTK, WordNet |
| ğŸ Programming | Python 3.11.4 |
| âš™ï¸ Deployment | WSL (Dev), Raspberry Pi OS (Prod) |
| ğŸ”§ Setup | Bash, `setup.sh`, `requirements.txt` |

---

## ğŸš€ Setup Instructions

### ğŸ“¦ Clone the Repository

```bash
git clone https://github.com/HetP1742431/AI-Powered-Voice-Assisted-Object-Locator.git
cd AI-Powered-Voice-Assisted-Object-Locator
```

### âš™ï¸ Run the Setup Script

```bash
chmod +x setup.sh
./setup.sh
```

This will:
- Install system dependencies
- Install Python 3.11.4 via pyenv
- Create and activate a virtual environment
- Install all Python dependencies
- Download YOLOv5m weights

### âœ… Run the Project

```bash
python3 src/main.py
```

## ğŸ“‹ Sample Usage Flow

User: "Hey Assistant, where is my black wallet?"
ğŸ§  â†’ Extracts: object=wallet, color=black
ğŸ¯ â†’ Scans environment and detects object
ğŸ™ï¸ â†’ Speaks: "Your black wallet is on the table, slightly to your left."

## ğŸ“ˆ Outcomes

- âœ… Successfully detects and filters household objects in cluttered scenes
- âœ… Accurate directional feedback with >90% recognition accuracy indoors
- âœ… Lightweight, end-to-end working prototype using affordable components
- âœ… Low setup time and portable form factor for real-world deployment

## ğŸ’¡ What I Learned
- ğŸ“¦ Gained hands-on experience with hardware-software integration using Raspberry Pi 5
- ğŸ§  Learned to build and optimize real-time object detection pipelines using YOLOv5 and OpenCV
- ğŸ—£ï¸ Understood the nuances of voice-to-intent mapping using spaCy, NLTK, and WordNet
- ğŸ§ª Practiced modular, testable Python design and deployment automation
- ğŸ”„ Improved debugging, cross-platform compatibility, and performance optimization for edge devices
- ğŸ¯ Gained confidence in translating user-centric problems into full-stack intelligent systems
